{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b476eb3",
   "metadata": {},
   "source": [
    "# Projet RAG (Retrivial Augmented Génération)\n",
    "\n",
    "\n",
    "#### Consigne \n",
    "\n",
    "Mini RAG : concevez un petit chatbot basé sur un modèle RAG. Vous utiliserez un jeu de documents simple, mono-source (PDF, site web, wiki personnel...) pour construire un pipeline minimal (embedding + vector search + modèle). Le but est d’afficher une réponse en langage naturel à une question posée. Vous pouvez utiliser LangChain ou tout autre outil équivalent.\n",
    "\n",
    "\n",
    "### J'utilise comme environnement virtuel *Poetry*\n",
    "\n",
    "#### 1 / Téléchargé *poetry*\n",
    "\n",
    "```bash\n",
    "    pip install poetry\n",
    "```\n",
    "\n",
    "#### 2 / Installer les dépendances via *poetry*\n",
    "\n",
    "```bash\n",
    "    poetry install\n",
    "```\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Utilisation des models via Ollama\n",
    "\n",
    "#### 1 / Installer Ollama sur votre ordinateur\n",
    "\n",
    "Commande windows\n",
    "```bash\n",
    "    winget install --id=Ollama.Ollama -e\n",
    "```\n",
    "\n",
    "#### 2 / Vérifier votre installation \n",
    "\n",
    "```bash\n",
    "    ollama --version\n",
    "    ollama serve\n",
    "```\n",
    "\n",
    "#### 3 / Télécharger un model \n",
    "\n",
    "```bash\n",
    "    ollama pull llama3:8b\n",
    "```\n",
    "\n",
    "#### Models possible \n",
    "\n",
    "\n",
    "```bash\n",
    "    # Liste des modèles à télécharger\n",
    "    MODELS=(\n",
    "    \"mistral:7b-instruct\"\n",
    "    \"llama3.2:1b\"\n",
    "    \"llama3.2:3b\"\n",
    "    \"llama3.1:8b-instruct\"\n",
    "    \"llama2:7b-chat\"\n",
    "    \"mixtral:8x7b\"\n",
    "    \"qwen2.5:7b-instruct\"\n",
    "    \"gemma2:2b-instruct\"\n",
    "    \"phi3:mini\"\n",
    ")\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Utilisation Streamlit pour l'interface \n",
    "\n",
    "```bash\n",
    "    streamlit run src/front/app.py\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Organisation Fonction \n",
    "\n",
    "#### ```embedding.py``` \n",
    "\n",
    "- load_text_file : load les datas présentes dans le dossier data\n",
    "\n",
    "- chunk_text : Séparer en chunks les textes extrait et les regroupes dans un tableau\n",
    "\n",
    "- class VectorIndex : "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
